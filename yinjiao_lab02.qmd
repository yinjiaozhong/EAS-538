---
embed-resources: true
---
---
title: "Lab 02 Central Limit Theorem"
author: "Yinjiao Zhong"
date: "2024-02-06"
output: html_document
---
***Lab Setup:***
Set your working directory, load in the firinfo.csv data
```{r setup}
setwd("/Users/zhongyinjiao/Desktop/EAS 538 Lab")
fireinfo <- read.csv("firinfo.csv")
head(fireinfo)
mean(fireinfo$BurnBndAc)
sd(fireinfo$BurnBndAc)
hist(fireinfo$BurnBndAc, probability = TRUE, main = "Distribution of Fire Burn Area", xlab='Burn Area (acre)')
```

# Exercise 1
```{r}
#get mean vector
meansVector <- function(data, times, size, var) {
  v <- c()
  for (i in 1:times) {
    y <- sample(data[, var], size, replace = TRUE)
    m <- mean(y)
    v[i] <- m
  }
  return(v)
}
```

## Question 1.1

 Explain what happens to the sampling distribution as you increase the number of subsamples you take.You should display four graphs: four sampling distributions with increasing numbers of subsamples.

```{r}
set.seed(100)
x <- c(10, 100, 1000, 10000) # number of subsamples

par(mfrow = c(2, 2))
for (i in c(1:length(x))) {
  means <- meansVector(fireinfo, x[i], 100, "BurnBndAc")
  avg <- round(mean(means), 5) # mean of the sample means
  SD <- round(sd(means), 5) # std of the sample means
  hist(means, probability = TRUE, main = paste0(x[i], " subsamples with 100 values each"), breaks = 30)
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)), bty = "n", text.col = c("blue", "dark green"))
}
```

**Answer:**
 
-When I increase the number of subsamples I take, the histograms looks more and more like a normal distribution.
-The mean of the sampling distribution is approaching the mean of the population.(7.8287)
-The standard deviation of the sampling distribution id decreading as the number of subsamples increases from 0.18 to 0.11.

## Question 1.2
 Explain what happens to the sampling distribution as you increase the number of values within each subsample.You should display four graphs: four sampling distributions with increasing numbers of values within each subsample.

```{r}
set.seed(100)
y <- c(10, 100, 1000, 5000)

par(mfrow = c(2, 2))
for (i in c(1:length(y))) {
  means <- meansVector(fireinfo, 1000, y[i], "BurnBndAc")
  avg <- round(mean(means), 5)
  SD <- round(sd(means), 5)
  hist(means, probability = TRUE, main = paste0("1000 subsamples with ", y[i], " values each"))
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)), bty = "n", text.col = c("blue", "dark green"))
}
```

**Answer:**

-When I increase the number of values within each subsample, the standard deviation is lower and lower.
-The sampling distribution is approaching normal distribution.
-When sample size>30, the sampling distribution is more likely normally distributed.

## Question 1.3
 How are the processes you described in questions 1 and 2 similar? How are they different?

**Answer:**
Similarity:
Both can contribute to enhancing the reliability of estimates for the population parameter.

Difference:
-Increasing the number of values in each sample will make each sample better represent the population, with the mean becoming more concentrated around the population mean, thus resulting in smaller deviations and greater precision.
-As the sample size increases, due to the Central Limit Theorem (CLT), the sampling distribution becomes closer to a normal distribution, leading to more precise estimates of the mean.

# Exercise 2

 Now demonstrate the central limit theorem on your own by creating a dataset called `"rentdata"

```{r}
#Follow the Instructions to create a dataset called "rentdata" on your own.
set.seed(100)
rentdata=rnorm(mean=1100,n=40000,sd=200)
```

This dataset represents the rent for every single undergaduate student at the University of Michigan (i.e. the full population). **You can use the `meansVector()` function to generate a vector of means.**

```{r}
meansVector <- function(data, times, size) {
  v <- c()
  for (i in 1:times) {
    y <- sample(data, size, replace=TRUE)
    m <- mean(y)
    v[i] <- m
  }
  return(v)
}
```


**Hint:** Make sure that you are able to (1) plot the histogram of the `"rentdata"` dataset; (2) create a series of histograms where you gradually increase the number of subsamples you take, but keep the number of values in each subsample constant; (3) create a series of histograms where you keep the number of subsamples you take constant, but increase the number of values within each subsample.

You should show one histgram and two sets of four graphs. Refer to your answers in 1.1 and 1.2 if you're looking for a place to start.

```{r}
set.seed(100)
# (1) Plot the histogram of the rent data
hist(rentdata,main="Histogram of Rent Data for UM students", xlab="Rent", probability = TRUE)
```


```{r}
# (2) Create a series of histograms where you gradually increase the number of subsamples you take, but keep the number of values in each subsample constant
meansVector <- function(data, times, size) {
  v <- c()
  for (i in 1:times) {
    y <- sample(data, size, replace=TRUE)
    m <- mean(y)
    v[i] <- m
  }
  return(v)
}
y <- c(10, 100, 1000, 10000)
par(mfrow = c(2,2))
for (i in c(1:length(y))) {
  means <- meansVector(rentdata, y[i], 30)
  avg <- round(mean(means), 5)
  SD <- round(sd(means), 5)
  hist(means, probability = TRUE, main = paste0(y[i],' subsubsamples with 50 values each'), xlab = 'mean rent', breaks = 50)
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)), bty = "n", text.col = c("blue", "dark green"))
}
```


```{r}
# (3) Create a series of histograms where you keep the number of subsamples you take constant, but increase the number of values within each subsample
meansVector <- function(data, times, size) {
  v <- c()
  for (i in 1:times) {
    y <- sample(data, size, replace=TRUE)
    m <- mean(y)
    v[i] <- m
  }
  return(v)
}
y <- c(10, 100, 500, 1000)
par(mfrow = c(2,2))
for (i in c(1:length(y))) {
  means <- meansVector(rentdata, 1000, y[i])
  avg <- round(mean(means), 5)
  SD <- round(sd(means), 5)
  hist(means, main = paste0('1000 subsamples with ', y[i], 'values each'), breaks=50, xlab='mean rent', probability=TRUE)
  legend("topright", legend = c(paste0("mean=", avg), paste0("SD=", SD)), bty = "n", text.col = c("blue","dark green"))
}
```


# Exercise 3
```{r}
set.seed(100)
# 10000 students' heights
heights <- rnorm(10000, mean = 65, sd = 2)
#CI
cifun <- function(means, zcrit, sem) {
  cilower <- means - zcrit*sem
  ciupper <- means + zcrit*sem
  civals <- c(cilower, ciupper)
  return(civals)
}
```


## Question 3.1

 Please interpret the meaning of the CIs you just calculated. What can you say about the true population parameter (e.g., mean height of all high school students)? Please include the code calculating the CIs.

```{r}
#get values of mean, zcrit, sem
means <- mean(heights)
zcrit <- qnorm(.975)
sem <- (sd(heights) / sqrt(length(heights)))
#compute CI
cifun(means, zcrit, sem)
```
**Answer:**
The lower bounds of CIs is 64.96876, it represents the maximum possible value of high school students' height at a 95% confidence level. And the upper bounds of CIs is 65.04675, it represents the minimum possible value of high school students' height at a 95% condidence level. And there is a 95% probability that the true mean height of all high school students is contained in the CI.

## Question 3.2

 Please calculate the 90% CIs. How do these differ from the 95% CIs you first calculated?

```{r}
#get values of mean, zcrit, sem
means <- mean(heights)
# CI is 90%
zcrit <- qnorm(.95)
sem <- (sd(heights) / sqrt(length(heights)))
#compute CI
cifun(means, zcrit, sem)
```
**Answer:**
90% confidence level means our confidence in the results is slightly lower, but the confidence interval is usually narrower. However, the 95% confidence level provides higher confidence, and the confidence interval may be wider.

## Question 3.3

 Say instead of sampling 10,000 students we only sampled 100. Calculate the 95% CIs of this new sample. How do they compare to the 95% CIs of the 10,000 sample data?

```{r}
set.seed(100)
heights <- rnorm(100, mean = 65, sd = 2)
cifun <- function(means, zcrit, sem) {
  cilower <- means - zcrit*sem
  ciupper <- means + zcrit*sem
  civals <- c(cilower, ciupper)
  return(civals)
}
means <- mean(heights)
zcrit <- qnorm(.975)
sem <- (sd(heights) / sqrt(length(heights)))
cifun(means, zcrit, sem)
```
**Answer:**
With 10,000 samples, more information is available, so the 95% confidence interval is narrower, providing higher precision and reliability. With 100 samples, less information is available, so the 95% confidence interval is wider, resulting in lower precision and reliability.